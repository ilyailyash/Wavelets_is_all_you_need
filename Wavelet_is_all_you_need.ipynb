{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "\n",
    "\n",
    "from scipy import fft\n",
    "import librosa as lib\n",
    "\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/ilya/workspace/ESC-50')\n",
    "from utils import ESC50\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "best_score = float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyError(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def __str__(self):\n",
    "        return repr(self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wavelet(nn.Module):\n",
    "    def __init__(self, num_layers, kernel_size, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, disp=False):\n",
    "        super(Wavelet, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.disp = disp\n",
    "        \n",
    "            \n",
    "        self.weight_hi = nn.Parameter(torch.Tensor(1,1,kernel_size))\n",
    "        self.weight_lo = nn.Parameter(torch.Tensor(1,1,kernel_size))\n",
    "        \n",
    "        self.weights = []\n",
    "        self.weights.append(self.weight_hi) \n",
    "        for i in range(1,num_layers):\n",
    "            self.weights.append(torch.Tensor(1,1,kernel_size*(2**i)))\n",
    "        self.weights.append(torch.Tensor(1,1,kernel_size*2**(num_layers-1)))\n",
    "        \n",
    "        self.weights_dec = copy.deepcopy(self.weights)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "# # Из фильтра высоких частот        \n",
    "    def reset_parameters(self):\n",
    "#         len1, len2 = math.ceil(self.kernel_size/2), math.floor(self.kernel_size/2)\n",
    "#         part_one = nn.Parameter(torch.Tensor(1,1,len1))\n",
    "#         part_two = nn.Parameter(torch.Tensor(1,1,len2))\n",
    "#         nn.init.xavier_uniform_(part_one)\n",
    "#         nn.init.xavier_uniform_(part_two)       \n",
    "        \n",
    "        \n",
    "#         part_one = part_one-part_one.mean()+2**(-1/2)/len1\n",
    "#         part_two = part_two-part_two.mean()+2**(-1/2)/len2\n",
    "        \n",
    "#         even = torch.arange(0, self.kernel_size, 2).long()\n",
    "#         odd = torch.arange(1, self.kernel_size, 2).long()\n",
    "                \n",
    "#         weight_lo = torch.Tensor(1,1,self.kernel_size)\n",
    "#         weight_lo[:,:,even] = part_one\n",
    "#         weight_lo[:,:,odd] = part_two      \n",
    "        \n",
    "#         del part_one\n",
    "#         del part_two\n",
    "        \n",
    "#         self.weight_lo = nn.Parameter(weight_lo/torch.sqrt(self.energy(weight_lo)))\n",
    "#         idx = torch.arange(self.kernel_size-1, -1, -1).long()\n",
    "#         weight_hi = self.weight_lo[:,:,idx]\n",
    "#         weight_hi[:,:,odd] = weight_hi[:,:,odd]*-1\n",
    "#         self.weight_hi = nn.Parameter(weight_hi)\n",
    "\n",
    "    \n",
    "        nn.init.xavier_uniform_(self.weight_hi)\n",
    "        weight_hi = self.weight_hi-self.weight_hi.mean()\n",
    "        self.weight_hi = nn.Parameter(weight_hi/torch.sqrt(self.energy(weight_hi)))\n",
    "        idx = torch.arange(self.weight_lo.size(2)-1, -1, -1).long()\n",
    "        weight_lo = self.weight_hi[:,:,idx]\n",
    "        odd = torch.arange(1, self.weight_lo.size(2)-1, 2).long()\n",
    "        weight_lo[:,:,odd] = weight_lo[:,:,odd]*-1\n",
    "        self.weight_lo = nn.Parameter(weight_lo)\n",
    "        \n",
    "    def energy(self, tensor):\n",
    "        return (tensor.pow(2).sum())\n",
    "    \n",
    "    def upsample(self, weigth, filt, filt_rec):\n",
    "        kernel_size = weigth.shape[-1]\n",
    "        filt_size = filt.shape[-1]\n",
    "        upsampeled = torch.zeros((1,1,kernel_size*2-1)).cuda() #cuda does't implements automaticly \n",
    "    \n",
    "        upsampeled[:,:,::2] = weigth\n",
    "        upsampeled_pad = F.pad(upsampeled,(math.ceil(filt_size/2),math.floor(filt_size/2))) #padding\n",
    "        idx = torch.arange(upsampeled_pad.size(2)-1, -1, -1).long()\n",
    "        return upsampeled, F.conv1d(upsampeled_pad, filt), F.conv1d(upsampeled_pad[:,:,idx], filt_rec)\n",
    "    \n",
    "    \n",
    "\n",
    "    def reset_weights_enc(self):\n",
    "        weight_lo, weight_hi = self.weight_hi,self.weight_lo\n",
    "        self.weights[0] = weight_hi\n",
    "        idx = torch.arange(weight_hi.size(2)-1, -1, -1).long()\n",
    "        self.weights_dec[0] = weight_hi[:,:,idx]\n",
    "        last_lo = weight_lo\n",
    "        last_hi = weight_hi\n",
    "        accumulated_lo = weight_lo\n",
    "        accumulated_lo_dec = weight_lo[:,:,idx]\n",
    "        \n",
    "        for i in range(1,self.num_layers):\n",
    "            last_hi, self.weights[i], self.weights_dec[i] = self.upsample(last_hi, accumulated_lo,\n",
    "                                                                        accumulated_lo_dec)\n",
    "            last_lo, accumulated_lo, accumulated_lo_dec = self.upsample(last_lo, accumulated_lo,\n",
    "                                                                        accumulated_lo_dec)\n",
    "        self.weights[self.num_layers] = accumulated_lo\n",
    "        self.weights_dec[self.num_layers] = accumulated_lo_dec\n",
    "        if self.disp:\n",
    "            for w in self.weights[:]:\n",
    "                print (self.energy(w.data))\n",
    "\n",
    "\n",
    "    def reset_weights_dec(self):\n",
    "        for i,weight in enumerate(self.weights):            \n",
    "            inv_idx = torch.arange(weight.size(2)-1, -1, -1).long()\n",
    "            self.weights_dec[i] = weight[:,:,inv_idx]\n",
    "\n",
    "\n",
    "    def encoding(self,data):\n",
    "        self.reset_weights_enc()\n",
    "        encoding = torch.Tensor(data.size(0),self.num_layers+1,data.size(2)).cuda()\n",
    "        for i,weight in enumerate(self.weights):\n",
    "            filt_size = weight.size(2)-1\n",
    "            left = math.ceil(filt_size/2)\n",
    "            rigth = math.floor(filt_size/2)\n",
    "            encoding[:,i,:] = F.conv1d(F.pad(data,(left,rigth)), weight).squeeze(1)\n",
    "        return encoding\n",
    "    \n",
    "    def decoding(self,encoding):\n",
    "#         self.reset_weights_dec()\n",
    "        decoding = torch.zeros((encoding.size(0),1,encoding.size(2))).cuda()\n",
    "        for i,weight in enumerate(self.weights_dec):\n",
    "            filt_size = weight.size(2)-1\n",
    "            left = math.floor(filt_size/2)\n",
    "            rigth = math.ceil(filt_size/2)\n",
    "            decoding += F.conv1d(F.pad(encoding[:,i,:].unsqueeze(1),(left,rigth)), weight)\n",
    "        return decoding\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,data):\n",
    "        self.reset_weights_enc()\n",
    "        encoding = torch.Tensor(data.size(0),self.num_layers+1,data.size(2)).cuda()\n",
    "        for i,weight in enumerate(self.weights):\n",
    "            filt_size = weight.size(2)-1\n",
    "            left = math.ceil(filt_size/2)\n",
    "            rigth = math.floor(filt_size/2)\n",
    "            encoding[:,i,:] = F.conv1d(F.pad(data,(left,rigth)), weight).squeeze(1)\n",
    "        return encoding\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self, wavelet_num_layers, wavelet_kernel_size, stride=1,\n",
    "                 padding=1, dilation=1, groups=1,disp = False):\n",
    "        super(classifier, self).__init__()\n",
    "        self.wavelet_num_layers = wavelet_num_layers\n",
    "        self.wavelet_kernel_size = wavelet_kernel_size\n",
    "        \n",
    "        self.wavelet = Wavelet(wavelet_num_layers, wavelet_kernel_size, disp=disp)\n",
    "        self.conv1 = nn.Conv1d(wavelet_num_layers+1, 15, 3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(15,20,3)\n",
    "        self.conv3 = nn.Conv1d(20,25,3)\n",
    "        self.lastpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(25,120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120,10)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        encoded_x = self.wavelet(x)\n",
    "        output = self.pool(self.conv1(encoded_x))\n",
    "        output = self.pool(self.conv2(output))\n",
    "        output = self.conv3(output)\n",
    "        output = self.lastpool(output)\n",
    "        output = self.relu(self.fc1(output.squeeze()))\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_splits = [1,2,3,4]\n",
    "test_split = 5\n",
    "\n",
    "shared_params = {'csv_path': '/home/ilya/workspace/ESC-50/esc50.csv',\n",
    "                 'wav_dir': '/home/ilya/workspace/ESC-50/audio',\n",
    "                 'dest_dir': '/home/ilya/workspace/ESC-50/audio/16000',\n",
    "                 'audio_rate': 16000,\n",
    "                 'only_ESC10': True,\n",
    "                 'pad': 0,\n",
    "                 'normalize': True}\n",
    "\n",
    "# train_gen = ESC50(folds=train_splits,\n",
    "#                   randomize=True,\n",
    "#                   strongAugment=True,\n",
    "#                   random_crop=True,\n",
    "#                   inputLength=2,\n",
    "#                   mix=False,\n",
    "#                   **shared_params).batch_gen(128)\n",
    "\n",
    "test_gen = ESC50(folds=[test_split],\n",
    "                 randomize=False,\n",
    "                 strongAugment=False,\n",
    "                 random_crop=False,\n",
    "                 inputLength=4,\n",
    "                 mix=False,\n",
    "                 **shared_params).batch_gen(128)\n",
    "\n",
    "# X, Y = next(train_gen)\n",
    "# X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (wavelet): Wavelet()\n",
       "  (conv1): Conv1d(6, 15, kernel_size=(3,), stride=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(15, 20, kernel_size=(3,), stride=(1,))\n",
       "  (conv3): Conv1d(20, 25, kernel_size=(3,), stride=(1,))\n",
       "  (lastpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc1): Linear(in_features=25, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = classifier(5,10)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/train')\n",
    "def criterion(outputs, labels, w_hi, w_lo, C):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    kernel_size = w_hi.size(2)\n",
    "    \n",
    "    L1 = (w_hi.pow(2).sum()-1).pow(2)\n",
    "    L2 = (w_lo.pow(2).sum()-1).pow(2)\n",
    "    en = L1+L2\n",
    "    \n",
    "    tmp = torch.zeros((1,1,1)).cuda()\n",
    "    for m in range(1,w_hi.size(2)//2):\n",
    "        prods = [w_hi[:,:,i]*w_hi[:,:,i+2*m] for i in range(kernel_size-2*m)]\n",
    "        for n in prods: tmp += n \n",
    "    L1 = tmp[0,0,0].pow(2) + L1\n",
    "    \n",
    "    tmp = torch.zeros((1,1,1)).cuda()\n",
    "    for m in range(1,w_hi.size(2)//2):\n",
    "        prods = [w_lo[:,:,i]*w_lo[:,:,i+2*m] for i in range(kernel_size-2*m)]\n",
    "        for n in prods: tmp += n \n",
    "    L2 = tmp[0,0,0].pow(2) + L2      \n",
    "\n",
    "    L3 = torch.zeros((1,1,1)).cuda()\n",
    "    prods = [w_hi[:,:,i]*-1 for i in range(1,kernel_size,2)]\n",
    "    for n in prods: L3 += n \n",
    "    L3 = L3[0,0,0].pow(2)\n",
    "    \n",
    "    L4 = (w_lo.sum()-2**(1/2)).pow(2)\n",
    "    \n",
    "#     L5 = torch.zeros((1,1,1)).cuda()\n",
    "#     for m in range(1,w_hi.size(2)//2):\n",
    "#         prods = [w_lo[:,:,i]*w_hi[:,:,i+2*m] for i in range(kernel_size-2*m)]\n",
    "#         for n in prods: L5 += n \n",
    "#     L5 = L5[0,0,0].pow(2) \n",
    "\n",
    "    CE_Loss = criterion(outputs, labels)\n",
    "    return (CE_Loss+C*(L1+L2+L3+L4), en, CE_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.95)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7,patience=200, verbose=True, threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C = 0.05\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    train_gen = ESC50(folds=train_splits,\n",
    "                  randomize=True,\n",
    "                  strongAugment=False,\n",
    "                  random_crop=False,\n",
    "                  inputLength=2,\n",
    "                  mix=False,\n",
    "                  **shared_params).batch_gen(50)\n",
    "    \n",
    "    test_gen = ESC50(folds=[test_split],\n",
    "                 randomize=True,\n",
    "                 strongAugment=False,\n",
    "                 random_crop=False,\n",
    "                 inputLength=4,\n",
    "                 mix=False,\n",
    "                 **shared_params).batch_gen(50)\n",
    "    \n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_gen):\n",
    "        # get the inputs\n",
    "        \n",
    "        inputs, labels = torch.Tensor(inputs).transpose(1,2).cuda(), torch.LongTensor(labels).cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "               \n",
    "        \n",
    "        _, labels = torch.max(labels, 1)\n",
    "        \n",
    "\n",
    "        acc = (outputs.max(1)[1]==labels).float().mean() \n",
    "        \n",
    "        loss, en, ce_loss = criterion(outputs, labels, net.wavelet.weight_hi, net.wavelet.weight_lo, C)\n",
    "        \n",
    "        writer.add_scalar(\"loss\", loss.item())\n",
    "        writer.add_scalar(\"en\", en.item())\n",
    "        writer.add_scalar(\"Acc\", acc.item())\n",
    "        \n",
    "        try:\n",
    "            if en > 15: raise EnergyError(en)\n",
    "        except EnergyError as e:\n",
    "            print ('Energy of filteres occurred to be:', e.value, ce_loss, ' step is ', i)\n",
    "            net = classifier(5,10)\n",
    "            net.load_state_dict(torch.load('saves/best_model'))\n",
    "            net.cuda()\n",
    "            continue\n",
    "            \n",
    "        scheduler.step(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print ('epoch = {}, iter = {}, energy = {:1.4}, CE_Loss = {:1.4}, Loss = {:1.4}'.format\n",
    "                   (epoch, i, en.item(), ce_loss.item(), loss.item()))\n",
    "                \n",
    "            idx = torch.arange(net.wavelet.weight_hi.size(2)-1, -1, -1).long()\n",
    "            hi_f = np.abs(fft(net.wavelet.weight_hi[0,0,idx].cpu().data.numpy()))\n",
    "            lo_f = np.abs(fft(net.wavelet.weight_lo[0,0,idx].cpu().data.numpy()))\n",
    "            n = hi_f.shape[-1]\n",
    "            plt.plot(range(n//2),hi_f[:n//2])\n",
    "            plt.plot(range(n//2),lo_f[:n//2]) \n",
    "            plt.show()\n",
    "                \n",
    "        if i%200 == 0:\n",
    "            inputs, labels = test_gen.__next__()\n",
    "            inputs, labels = torch.Tensor(inputs).transpose(1,2).cuda(), torch.LongTensor(labels).cuda()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             _, labels = torch.max(labels, 1)\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(labels.detach().cpu().numpy(),outputs.detach().cpu().numpy())\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            if best_score < roc_auc:\n",
    "                best_score = roc_auc\n",
    "                torch.save(net.state_dict(),'saves/best_model')\n",
    "            \n",
    "            writer.add_scalar(\"ROC AUC Val\", roc_auc.item())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = classifier(5,10)\n",
    "net.load_state_dict(torch.load('saves/last_model5_10'))\n",
    "net.cuda()\n",
    "net.wavelet.reset_weights_enc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi,lo = net.wavelet.weight_hi, net.wavelet.weight_lo \n",
    "\n",
    "idx = torch.arange(hi.size(2)-1, -1, -1).long() \n",
    "hi_f,lo_f = np.abs(fft(hi[0,0,idx].cpu().data.numpy())), np.abs(fft(lo[0,0,idx].cpu().data.numpy())) \n",
    "n = hi_f.shape[-1] \n",
    "plt.grid(True) \n",
    "plt.axis([0, 1, 0, 2.5]) \n",
    "plt.plot(np.arange(n//2+1)/(n//2),lo_f[:n//2+1]) \n",
    "plt.plot(np.arange(n//2+1)/(n//2),hi_f[:n//2+1]) \n",
    "plt.xlabel(r'$\\frac{2f}{f_д}$', fontsize=20) \n",
    "plt.ylabel('Амплитуда') \n",
    "plt.savefig('learned.png', dpi=100) \n",
    "plt.show() \n",
    "for i,w in enumerate(net.wavelet.weights): \n",
    "    plt.grid(True) \n",
    "    idx = torch.arange(w.size(2)-1, -1, -1).long() \n",
    "    f = np.abs(fft(w[0,0,idx].cpu().data.numpy())) \n",
    "    n = w.shape[-1] \n",
    "    plt.axis([0, 1, 0, 30]) \n",
    "    plt.xlabel(r'$\\frac{2f}{f_д}$', fontsize=20) \n",
    "    plt.ylabel('Амплитуда') \n",
    "    plt.plot(np.array(range(n//2+1))/(n//2),f[:n//2+1]) \n",
    "    plt.savefig('layerslearned{}.png'.format(i)) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "test_gen = ESC50(folds=[test_split],\n",
    "                 randomize=False,\n",
    "                 strongAugment=False,\n",
    "                 random_crop=False,\n",
    "                 inputLength=4,\n",
    "                 mix=False,\n",
    "                 **shared_params).batch_gen(50)\n",
    "\n",
    "\n",
    "for inputs, labels in test_gen:\n",
    "        # get the inputs\n",
    "        inputs, labels = torch.Tensor(inputs).transpose(1,2).cuda(), torch.LongTensor(labels).cuda()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        \n",
    "#         loss = criterion(outputs, labels, net.wavelet.weight_hi, net.wavelet.weight_lo, C, l2)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        clear_output(wait=1)\n",
    "        print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
